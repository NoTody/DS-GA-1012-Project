{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a75b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  6 17:47:35 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    40W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7cd826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available devices\n",
    "import torch\n",
    "max(1, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f302fe98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Available GPUs in current node=2\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Global seed set to 42\n",
      "Available GPUs in current node=2\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Encoding data ...\n",
      "Encoding data ...\n",
      "Encoding data ...\n",
      "Encoding data ...\n",
      "total step: 343.0\n",
      "total step: 343.0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name     | Type                          | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model    | BertForSequenceClassification | 109 M \n",
      "1 | accuracy | Accuracy                      | 0     \n",
      "2 | f1       | F1Score                       | 0     \n",
      "-----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "218.971   Total estimated model params size (MB)\n",
      "Global seed set to 42                                                           \n",
      "Global seed set to 42\n",
      "Epoch 0:  92%|█████████▏| 1720/1876 [07:24<00:40,  3.87it/s, loss=1.53, v_num=2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1740/1876 [07:26<00:34,  3.90it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1760/1876 [07:27<00:29,  3.93it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1780/1876 [07:29<00:24,  3.96it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1800/1876 [07:30<00:19,  3.99it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1820/1876 [07:32<00:13,  4.02it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1840/1876 [07:33<00:08,  4.06it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1860/1876 [07:35<00:03,  4.09it/s, loss=1.53, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|▉| 1860/1876 [07:36<00:03,  4.08it/s, loss=1.49, v_num=2, val_acc=\u001b[A\n",
      "Epoch 0: 100%|█| 1876/1876 [07:36<00:00,  4.11it/s, loss=1.49, v_num=2, val_acc=Epoch 0, global step 1718: val_f1 reached 0.24390 (best 0.24390), saving model to \"/gpfs/data/denizlab/Users/hh2740/NLU/DS-GA-1012-Project/lightning_logs/version_2/checkpoints/epoch=0-step=1718.ckpt\" as top 1\n",
      "/gpfs/share/skynet/apps/anaconda3/envs/opence_env_1.2.0_pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "/gpfs/share/skynet/apps/anaconda3/envs/opence_env_1.2.0_pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "Epoch 1:  92%|▉| 1720/1876 [07:16<00:39,  3.94it/s, loss=0.327, v_num=2, val_acc\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  93%|▉| 1740/1876 [07:18<00:34,  3.97it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  94%|▉| 1760/1876 [07:19<00:28,  4.00it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  95%|▉| 1780/1876 [07:21<00:23,  4.03it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  96%|▉| 1800/1876 [07:22<00:18,  4.06it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  97%|▉| 1820/1876 [07:24<00:13,  4.10it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  98%|▉| 1840/1876 [07:25<00:08,  4.13it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  99%|▉| 1860/1876 [07:27<00:03,  4.16it/s, loss=0.327, v_num=2, val_acc\u001b[A\n",
      "Epoch 1:  99%|▉| 1860/1876 [07:28<00:03,  4.15it/s, loss=0.317, v_num=2, val_acc\u001b[A\n",
      "Epoch 1: 100%|█| 1876/1876 [07:28<00:00,  4.18it/s, loss=0.317, v_num=2, val_accEpoch 1, global step 3437: val_f1 reached 0.90930 (best 0.90930), saving model to \"/gpfs/data/denizlab/Users/hh2740/NLU/DS-GA-1012-Project/lightning_logs/version_2/checkpoints/epoch=1-step=3437.ckpt\" as top 1\n",
      "Epoch 2:  92%|▉| 1720/1876 [07:18<00:39,  3.92it/s, loss=0.178, v_num=2, val_acc\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  93%|▉| 1740/1876 [07:20<00:34,  3.95it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  94%|▉| 1760/1876 [07:21<00:29,  3.99it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  95%|▉| 1780/1876 [07:23<00:23,  4.02it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  96%|▉| 1800/1876 [07:24<00:18,  4.05it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  97%|▉| 1820/1876 [07:26<00:13,  4.08it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  98%|▉| 1840/1876 [07:27<00:08,  4.11it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  99%|▉| 1860/1876 [07:28<00:03,  4.14it/s, loss=0.178, v_num=2, val_acc\u001b[A\n",
      "Epoch 2:  99%|▉| 1860/1876 [07:30<00:03,  4.13it/s, loss=0.186, v_num=2, val_acc\u001b[A\n",
      "Epoch 2: 100%|█| 1876/1876 [07:30<00:00,  4.17it/s, loss=0.186, v_num=2, val_accEpoch 2, global step 5156: val_f1 reached 0.92750 (best 0.92750), saving model to \"/gpfs/data/denizlab/Users/hh2740/NLU/DS-GA-1012-Project/lightning_logs/version_2/checkpoints/epoch=2-step=5156.ckpt\" as top 1\n",
      "Epoch 3:  92%|▉| 1720/1876 [07:20<00:39,  3.90it/s, loss=0.161, v_num=2, val_acc\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  93%|▉| 1740/1876 [07:22<00:34,  3.93it/s, loss=0.161, v_num=2, val_acc\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  94%|▉| 1760/1876 [07:23<00:29,  3.96it/s, loss=0.161, v_num=2, val_acc\u001b[A\n",
      "Epoch 3:  95%|▉| 1780/1876 [07:25<00:24,  4.00it/s, loss=0.161, v_num=2, val_acc\u001b[A\n",
      "Epoch 3:  96%|▉| 1800/1876 [07:26<00:18,  4.03it/s, loss=0.161, v_num=2, val_acc\u001b[A\n",
      "Epoch 3:  97%|▉| 1820/1876 [07:28<00:13,  4.06it/s, loss=0.161, v_num=2, val_acc\u001b[A\n",
      "Epoch 3:  98%|▉| 1840/1876 [07:29<00:08,  4.09it/s, loss=0.161, v_num=2, val_acc\u001b[A\n",
      "Epoch 3:  99%|▉| 1860/1876 [07:31<00:03,  4.12it/s, loss=0.161, v_num=2, val_acc\u001b[A\n",
      "Epoch 3:  99%|▉| 1860/1876 [07:32<00:03,  4.11it/s, loss=0.17, v_num=2, val_acc=\u001b[A\n",
      "Epoch 3: 100%|█| 1876/1876 [07:32<00:00,  4.15it/s, loss=0.17, v_num=2, val_acc=Epoch 3, global step 6875: val_f1 reached 0.93030 (best 0.93030), saving model to \"/gpfs/data/denizlab/Users/hh2740/NLU/DS-GA-1012-Project/lightning_logs/version_2/checkpoints/epoch=3-step=6875.ckpt\" as top 1\n",
      "Epoch 4:  92%|▉| 1720/1876 [07:20<00:39,  3.90it/s, loss=0.123, v_num=2, val_acc\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  93%|▉| 1740/1876 [07:22<00:34,  3.93it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  94%|▉| 1760/1876 [07:24<00:29,  3.96it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  95%|▉| 1780/1876 [07:25<00:24,  3.99it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  96%|▉| 1800/1876 [07:27<00:18,  4.02it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  97%|▉| 1820/1876 [07:28<00:13,  4.06it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  98%|▉| 1840/1876 [07:30<00:08,  4.09it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  99%|▉| 1860/1876 [07:31<00:03,  4.12it/s, loss=0.123, v_num=2, val_acc\u001b[A\n",
      "Epoch 4:  99%|▉| 1860/1876 [07:32<00:03,  4.11it/s, loss=0.172, v_num=2, val_acc\u001b[A\n",
      "Epoch 4: 100%|█| 1876/1876 [07:32<00:00,  4.14it/s, loss=0.172, v_num=2, val_accEpoch 4, global step 8594: val_f1 reached 0.93220 (best 0.93220), saving model to \"/gpfs/data/denizlab/Users/hh2740/NLU/DS-GA-1012-Project/lightning_logs/version_2/checkpoints/epoch=4-step=8594.ckpt\" as top 1\n",
      "Epoch 4: 100%|█| 1876/1876 [07:34<00:00,  4.13it/s, loss=0.172, v_num=2, val_acc\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "!python3 python ./main.py --accumulate_grad_batches 1 --num_nodes 1 --num_devices 2 --model_name \"bert-base-uncased\" --dataset_name \"agnews\" --num_workers 10 --max_epochs 10 --batch_size 32 --max_seq_length 256 --mode \"train\" --lr 2e-5 --num_labels 4 --scheduler_name \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "209fcf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Available GPUs in current node=2\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "test\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Test mode\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Global seed set to 42\n",
      "Available GPUs in current node=2\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "test\n",
      "Test mode\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Encoding data ...\n",
      "Encoding data ...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Testing: 100%|████████████████████████████████| 119/119 [00:10<00:00, 12.74it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9422368407249451, 'test_f1': 0.9422368407249451}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|████████████████████████████████| 119/119 [00:10<00:00, 11.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "!python3 ./main.py --dataset_name \"agnews\" --num_labels 4 --accumulate_grad_batches 1 --num_nodes 1 --model_name \"bert-base-uncased\" --num_devices 2 --mode \"test\" --load_path \"/gpfs/data/denizlab/Users/hh2740/NLU/DS-GA-1012-Project/lightning_logs/agnews_bert_base/checkpoints/epoch=4-step=8594.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a3d0fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277861c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
