{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c915924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "from pytorch_lightning import Trainer, LightningModule, seed_everything\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, random_split\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from data_utils import *\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15caa246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, C, H, W = 20, 5, 8, 10\n",
    "input = torch.randn(N, C, H, W)\n",
    "input.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, args):\n",
    "\n",
    "        super().__init__(hparams)\n",
    "        # Set our init args as class attributes\n",
    "        self.hparams.update(vars(hparams))\n",
    "        \n",
    "        # Build models\n",
    "        __build_model()\n",
    "\n",
    "        # Define metrics\n",
    "        self.accuracy = Accuracy()\n",
    "        self.f1 = F1Score()\n",
    "    \n",
    "    #############################\n",
    "    # Training / Validation HOOKS\n",
    "    ############################# \n",
    "    def __build_model():\n",
    "        # Define tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_name, \n",
    "                                                       use_fast=True)\n",
    "\n",
    "        # Define config\n",
    "        self.config = AutoConfig.from_pretrained(self.hparams.model_name, \n",
    "                                                 num_labels=self.hparams.num_labels,\n",
    "                                                 output_hidden_states=True)\n",
    "        # Define model\n",
    "        self.student = model_init(self.hparams.model_name, self.config)\n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        # there is no backpropagation through the teacher, so no need for gradients\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _EMA_update(self, it):\n",
    "        \"\"\"\n",
    "        Exponential Moving Average update of the student\n",
    "        \"\"\"\n",
    "        m = self.momentum_schedule[it]  # momentum parameter\n",
    "        for param_q, param_k in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "\n",
    "    #############################\n",
    "    # Training / Validation HOOKS\n",
    "    #############################\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def forward_one_epoch(self, batch, batch_idx):\n",
    "        b_input_ids, b_attn_mask, b_labels = batch['ori']['input_ids'], batch['ori']['attention_mask'], batch['ori']['labels']\n",
    "        outputs_ori = self.student(b_input_ids, b_attn_mask)\n",
    "        \n",
    "        b_input_ids, b_attn_mask, b_labels = batch['str_adv']['input_ids'], batch['str_adv']['attention_mask'], batch['str_adv']['labels']\n",
    "        outputs_str_adv = self.teacher(b_input_ids, b_attn_mask)\n",
    "        \n",
    "        b_input_ids, b_attn_mask, b_labels = batch['weak_aug']['input_ids'], batch['weak_aug']['attention_mask'], batch['weak_aug']['labels']\n",
    "        outputs_weak_aug = self.student(b_input_ids, b_attn_mask)\n",
    "        \n",
    "        hidden_states_weak_aug = outputs_weak_aug.hidden_states\n",
    "        hidden_states_str_adv = outputs_str_adv.hidden_states\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # calcualate self-supervised loss from cls embeddings\n",
    "        for i in range(self.top_k_layers - 1):\n",
    "            cur_hidden = hidden_states_weak_aug[i][:, 0, :]\n",
    "            y_aug = torch.stack([y, cur_hidden], dim=1)\n",
    "            cur_hidden = hidden_states_str_adv[i][:, 0, :]\n",
    "            y_adv = torch.stack([y, cur_hidden], dim=1)\n",
    "        # layernorm\n",
    "        y_aug = F.layer_norm(y_aug, y_aug.shape[1:])\n",
    "        y_adv = F.layer_norm(y_adv, y_adv.shape[1:])\n",
    "        # calculate smooth l1 loss\n",
    "        sz = y_aug.size(-1)\n",
    "        loss_scale = 1 / math.sqrt(sz)\n",
    "        selfsup_loss = loss_scale * F.smooth_l1_loss(y_aug.float(), y_adv.float(), reduction=\"none\", \n",
    "                                                     beta=self.loss_beta).sum(dim=-1).sum()\n",
    "        # calculate supervised loss from true label\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        logits = outputs_ori.logits\n",
    "        sup_loss = criterion(logits, b_labels)\n",
    "        # calculate final loss\n",
    "        loss = (1 - self.hparams.lambda) * selfsup_loss + self.hparams.lambda * sup_loss\n",
    "        # get predict\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return {'loss': loss, 'preds': preds, 'labels': b_labels}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        forward_outputs = self.forward_one_epoch(batch, batch_idx)\n",
    "        train_loss = forward_outputs['loss']\n",
    "        b_input_ids = forward_outputs['input_ids']\n",
    "        # Tensorboard logging for model graph and loss\n",
    "        #self.logger.experiment.add_graph(self.model, input_to_model=b_input_ids, verbose=False, use_strict_trace=True)\n",
    "        #self.logger.experiment.add_scalars('loss', {'train_loss': train_loss}, self.global_step)\n",
    "        self.log(\"train_loss\", train_loss, on_epoch=False, on_step=True, prog_bar=True)\n",
    "        return train_loss\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        # update student/teacher with EMA after each batch\n",
    "        it = self.global_step\n",
    "        self._EMA_update(it)\n",
    "        \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         forward_outputs = self.forward_one_epoch(batch, batch_idx)\n",
    "#         val_loss = forward_outputs['loss']\n",
    "#         preds = forward_outputs['preds']\n",
    "#         labels = forward_outputs['labels']\n",
    "#         self.accuracy(preds, labels)\n",
    "#         self.f1(preds, labels)\n",
    "#         # Calling self.log will surface up scalars for you in TensorBoard\n",
    "#         #self.logger.experiment.add_scalars('loss', {'val_loss': val_loss}, self.global_step)\n",
    "#         self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # self.log(\"val_acc\", self.accuracy, on_epoch=True, on_step=False, prog_bar=True)\n",
    "#         # self.log(\"val_f1\", self.f1, on_epoch=True, on_step=False, prog_bar=True)\n",
    "#         self.log(\"val_acc\", self.accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_f1\", self.f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         return val_loss\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         forward_outputs = self.forward_one_epoch(batch, batch_idx)\n",
    "#         preds = forward_outputs['preds']\n",
    "#         b_labels = forward_outputs['labels']\n",
    "#         test_loss = forward_outputs['loss']\n",
    "#         #cls_hidden_states = forward_outputs['hidden_states'][0][:, 0, :]\n",
    "#         # Reuse the validation_step for testing\n",
    "#         # Visualize dimensionality reduced labels\n",
    "#         # print(cls_hidden_states.shape)\n",
    "#         # print(b_labels.shape)\n",
    "#         #self.logger.experiment.add_embedding(cls_hidden_states, metadata=b_labels.tolist(), global_step=self.global_step)\n",
    "#         self.accuracy(preds, b_labels)\n",
    "#         self.f1(preds, b_labels)\n",
    "#         self.log(\"test_acc\", self.accuracy)\n",
    "#         self.log(\"test_f1\", self.f1)\n",
    "#         return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # set no decay for bias and normalziation weights\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.student.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.student.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        # define optimizer / scheduler\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.learning_rate)\n",
    "        self.warmup_steps = 0.06 * self.total_steps\n",
    "        if self.hparams.scheduler_name == \"cosine\":\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.warmup_steps,\n",
    "                num_training_steps=self.total_steps,\n",
    "            )\n",
    "        elif self.hparams.scheduler_name == \"linear\":\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.warmup_steps,\n",
    "                num_training_steps=self.total_steps,\n",
    "            )\n",
    "        #scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "    def setup(self, stage=None):\n",
    "      # dataset setup\n",
    "      if stage == \"fit\" or stage is None:\n",
    "        # train dataset assign\n",
    "        train_path_ori = \"./datasets/\" + self.hparams.dataset_name_ori + \"_train.csv\"\n",
    "        train_path_str_adv = \"./datasets/\" + self.hparams.dataset_name_str_adv + \"_train.csv\"\n",
    "        train_path_weak_aug = \"./datasets/\" + self.hparams.dataset_name_weak_aug + \"_train.csv\"\n",
    "        # read/generate three ways dataset\n",
    "        df_train_ori = pd.read_csv(train_path)\n",
    "        df_train_str_adv = pd.read_csv(train_path)\n",
    "        df_train_weak_aug = pd.read_csv(train_path)\n",
    "        \n",
    "        self.ds_train_ori = SequenceDataset(df_train_ori, self.dataset_name, self.tokenizer,\n",
    "                                            max_seq_length=self.max_seq_length)\n",
    "        self.ds_train_weak_aug = SequenceDataset(df_train_weak_aug, self.dataset_name, self.tokenizer,\n",
    "                                                 max_seq_length=self.max_seq_length)\n",
    "        self.ds_train_str_adv = SequenceDataset(df_train_str_adv, self.dataset_name, self.tokenizer,\n",
    "                                                max_seq_length=self.max_seq_length)\n",
    "        \n",
    "        # val dataset assign\n",
    "        val_path = \"./datasets/\" + self.dataset_name + \"_val.csv\"\n",
    "        df_val = pd.read_csv(val_path)\n",
    "        self.ds_val = SequenceDataset(df_val, self.dataset_name, self.tokenizer, max_seq_length=self.max_seq_length)\n",
    "        # Calculate total steps\n",
    "        tb_size = self.batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(df_train) // tb_size) // ab_size\n",
    "        print(f\"total step: {self.total_steps}\")\n",
    "        \n",
    "    if stage == \"test\" or stage is None:\n",
    "        # test dataset assign\n",
    "        test_path = \"./datasets/\" + self.dataset_name + \"_test.csv\"\n",
    "        df_test = pd.read_csv(test_path)\n",
    "        self.ds_test = SequenceDataset(df_test, self.dataset_name, self.tokenizer, max_seq_length=self.max_seq_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.ds_train_ori = DataLoader(self.ds_train_ori, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "        ds_train_weak_aug = DataLoader(self.ds_train_weak_aug, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "        ds_train_str_adv = DataLoader(self.ds_train_str_adv, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "        \n",
    "        # momentum parameter is increased to 1. during training with a cosine schedule\n",
    "        self.momentum_schedule = model_utils.cosine_scheduler(self.hparams.momentum_teacher, 1,\n",
    "                                                              self.hparams.max_epochs, len(self.ds_train_ori))\n",
    "        \n",
    "        return {\"ori\": self.ds_train_ori, \"weak_aug\": ds_train_weak_aug, \"str_adv\": ds_train_str_adv}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser, root_dir):\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser])\n",
    "        # config parameters\n",
    "        parser.add_argument(\"--accumulate_grad_batches\", type=int, default=1)\n",
    "        parser.add_argument(\"--model_name\", type=str, default=\"bert-base-uncased\")\n",
    "        parser.add_argument(\"--dataset_name\", type=str, default=\"agnews\")\n",
    "        parser.add_argument(\"--num_workers\", type=int, default=10)\n",
    "        parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "        parser.add_argument(\"--max_seq_length\", type=int, default=256)\n",
    "        parser.add_argument(\"--mode\", type=str, default=\"train\")\n",
    "        parser.add_argument(\"--load_path\", type=str, default=None)\n",
    "        # model parameters\n",
    "        parser.add_argument(\"--lr\", type=float, default=2e-5)\n",
    "        parser.add_argument(\"--num_labels\", type=int, default=4)\n",
    "        parser.add_argument(\"--lambda\", type=float, default=0.5)\n",
    "        parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
    "        parser.add_argument('--momentum_teacher', default=0.996, type=float)\n",
    "        parser.add_argument(\"--scheduler_name\", type=str, default=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hparams):\n",
    "    \"\"\"\n",
    "    Main training routine specific for this project\n",
    "    :param hparams:\n",
    "    \"\"\"\n",
    "    # ------------------------\n",
    "    # 1 INIT LIGHTNING MODEL\n",
    "    # ------------------------\n",
    "    model = LitTransformer(hparams)\n",
    "    \n",
    "    # ------------------------\n",
    "    # 2 INIT CALLBACKS\n",
    "    # ------------------------\n",
    "    bar = TQDMProgressBar(refresh_rate=20, process_position=0)\n",
    "    early_stop_callback = EarlyStopping(monitor=\"loss\", min_delta=0.00, patience=2, verbose=False, mode=\"min\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='loss',\n",
    "        mode='min',\n",
    "        save_weights_only=False\n",
    "    )\n",
    "    \n",
    "    # Define Logger\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(save_dir=hparams.tb_save_dir) \n",
    "\n",
    "    # ------------------------\n",
    "    # 2 INIT TRAINER\n",
    "    # ------------------------\n",
    "    trainer = Trainer(precision=hparams.precision, gpus=hparams.gpus, accelerator=\"gpu\", num_nodes=hparams.num_nodes,\n",
    "                strategy=DDPStrategy(find_unused_parameters=False), max_epochs=hparams.max_epochs, \n",
    "                logger=tb_logger, callbacks=[checkpoint_callback, early_stop_callback, bar]\n",
    "                )\n",
    "\n",
    "    # ------------------------\n",
    "    # 3 START TRAINING\n",
    "    # ------------------------\n",
    "    trainer.fit(model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ------------------------\n",
    "    # TRAINING ARGUMENTS\n",
    "    # ------------------------\n",
    "    # these are project-wide arguments\n",
    "\n",
    "    root_dir = os.path.dirname('./trans_pl')\n",
    "    parent_parser = argparse.ArgumentParser(add_help=False)\n",
    "\n",
    "    # gpu args\n",
    "    parent_parser.add_argument(\n",
    "        '--gpus',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='how many gpus'\n",
    "    )\n",
    "    parent_parser.add_argument(\n",
    "        '--num_nodes',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='how many nodes'\n",
    "    )\n",
    "    parent_parser.add_argument(\n",
    "        '--precision',\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help='default to use mixed precision 16'\n",
    "    )\n",
    "    parent_parser.add_argument(\"--tb_save_dir\", \n",
    "                               type=str, \n",
    "                               default=\"../\",\n",
    "                               help='tensorboard save directory'\n",
    "                              )\n",
    "\n",
    "    # each LightningModule defines arguments relevant to it\n",
    "    parser = LitTransformer.add_model_specific_args(parent_parser)\n",
    "    hyperparams = parser.parse_args()\n",
    "  \n",
    "    # ---------------------\n",
    "    # RUN TRAINING\n",
    "    # ---------------------\n",
    "    main(hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
